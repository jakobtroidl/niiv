{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, query_dim, use_positional_embedding=True):\n",
    "        \"\"\"\n",
    "        embed_dim: Feature dimensionality (C)\n",
    "        num_heads: Number of attention heads\n",
    "        query_dim: Number of queries (e.g., 128 * 128 for high-res queries)\n",
    "        use_positional_embedding: If True, use positional embeddings as queries; else, use learnable query tokens.\n",
    "        \"\"\"\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.use_positional_embedding = use_positional_embedding\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        if use_positional_embedding:\n",
    "            # Use positional embeddings as queries\n",
    "            self.query_input = nn.Parameter(torch.randn(1, query_dim, embed_dim))  # (1, 128*128, C)\n",
    "        else:\n",
    "            # Use learnable query tokens instead\n",
    "            self.query_tokens = nn.Parameter(torch.randn(1, query_dim, embed_dim))  # (1, 128*128, C)\n",
    "\n",
    "    def forward(self, encoder_out):\n",
    "        \"\"\"\n",
    "        encoder_out: (B, 16 * 128, C) - Encoder feature grid.\n",
    "        Returns: (B, 128 * 128, C) - Updated feature map after cross-attention.\n",
    "        \"\"\"\n",
    "        B = encoder_out.shape[0]\n",
    "\n",
    "        if self.use_positional_embedding:\n",
    "            # Expand positional embeddings to match batch size\n",
    "            queries = self.query_input.expand(B, -1, -1)  # (B, 128*128, C)\n",
    "        else:\n",
    "            # Expand learnable query tokens\n",
    "            queries = self.query_tokens.expand(B, -1, -1)  # (B, 128*128, C)\n",
    "\n",
    "        # Apply cross-attention: Query attends over the encoder features\n",
    "        output, _ = self.attn(queries, encoder_out, encoder_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Positional Embedding: torch.Size([2, 16384, 256])\n",
      "With Learnable Query Tokens: torch.Size([2, 16384, 256])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "B, H_kv, W_kv, H_q, W_q, C, num_heads = 2, 16, 128, 128, 128, 256, 8\n",
    "query_dim = H_q * W_q  # 128 * 128\n",
    "\n",
    "# Encoder feature grid\n",
    "F = torch.randn(B, H_kv * W_kv, C)  # (B, 16*128, C)\n",
    "\n",
    "# Option 1: With Positional Embedding\n",
    "cross_attn_with_pos = CrossAttention(embed_dim=C, num_heads=num_heads, query_dim=query_dim, use_positional_embedding=True)\n",
    "I_updated_with_pos = cross_attn_with_pos(F)\n",
    "print(\"With Positional Embedding:\", I_updated_with_pos.shape)  # Expected: (B, 128*128, C)\n",
    "\n",
    "# Option 2: With Learnable Query Tokens\n",
    "cross_attn_with_tokens = CrossAttention(embed_dim=C, num_heads=num_heads, query_dim=query_dim, use_positional_embedding=False)\n",
    "I_updated_with_tokens = cross_attn_with_tokens(F)\n",
    "print(\"With Learnable Query Tokens:\", I_updated_with_tokens.shape)  # Expected: (B, 128*128, C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "niiv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
